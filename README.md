# Ollama QA

## ðŸš€ Why Choose Ollama?

Ollama empowers users to execute models locally, streamlining setup complexities and optimizing GPU usage. The Ollama model library offers a diverse range of supported models and variants.

Credits to [Ollama](https://ollama.ai/blog/ollama-is-now-available-as-an-official-docker-image).


## ðŸš€ Getting Started

1. Clone the repository.
2. Install the model and Python requirements.